# Step 1: Import Necessary Libraries

import os
import warnings
from google.colab import userdata
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
#from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.llms import LlamaCpp
from langchain.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")


# Step 2: Configure Google API Key

# Load the API key from Colab secrets for security
#os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')


# Step 3: Load the PDF Documents

# List the paths to your uploaded textbooks
pdf_paths = ["/content/textbook1.pdf", "/content/textbook2.pdf", "/content/textbook3.pdf"]
all_documents = []

print("Loading documents...")
for path in pdf_paths:
    try:
        loader = PyPDFLoader(path)
        documents = loader.load()
        all_documents.extend(documents)
        print(f"Successfully loaded {len(documents)} pages from {path}")
    except Exception as e:
        print(f"Error loading {path}: {e}")

if not all_documents:
    print("No documents were loaded. Please check the file paths and ensure the files are uploaded correctly.")
else:
    print(f"\nTotal pages loaded from all documents: {len(all_documents)}")


    # Step 4: Split Documents into Manageable Chunks

    print("\nSplitting documents into chunks...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # The size of each chunk in characters
        chunk_overlap=100 # Number of characters to overlap between chunks
    )
    docs = text_splitter.split_documents(all_documents)
    print(f"Successfully split documents into {len(docs)} chunks.")




    # Step 5: Create Embeddings and Store them in a FAISS Vector Database
    
    print("\nCreating embeddings and building the vector store... (This may take a few minutes)")
    # Using a popular open-source model for creating embeddings
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # Create the vector store from our document chunks and embedding model
    vector_store = FAISS.from_documents(docs, embedding_model)
    print("Vector store created successfully.")

    
    # Step 6: Initialize the Language Model (LLM)
    
    # We will use Google's Gemini Pro model for generation
    #llm = ChatGoogleGenerativeAI(model="gemini-pro")
    llm = LlamaCpp(
    model_path="/content/mistral.gguf",  # Path to the downloaded GGUF model
    n_ctx=2048,                          # Number of context tokens
    temperature=0.3,                     # Creativity level
    top_p=0.95,                          # Nucleus sampling
    n_gpu_layers=20,                     # Use GPU acceleration if available (set 0 for CPU-only)
    verbose=True                         # Optional: logs generation details
)
    print("\nLanguage model initialized.")

   
    # Step 7: Define the Prompt and Create the RAG Chain
   
    # This prompt template guides the LLM on how to answer the question
    # It instructs the model to use ONLY the provided context from the textbooks.
    prompt = ChatPromptTemplate.from_template("""
    You are a helpful legal studies assistant. Answer the user's question based *only* on the following context.
    If the context does not contain the answer, state that you cannot find the answer in the provided textbooks.
    Do not make up information. Be concise and clear in your response.

    Context:
    {context}

    Question: {input}

    Answer:
    """)

    # Create a chain that will pass the retrieved documents and the question to the LLM
    document_chain = create_stuff_documents_chain(llm, prompt)

    # The retriever's job is to fetch relevant documents from the vector store
    retriever = vector_store.as_retriever()

    # The final retrieval chain that combines the retriever and the document_chain
    retrieval_chain = create_retrieval_chain(retriever, document_chain)
    print("RAG chain created successfully.")

   
    # Step 8: Define a Query Function
 
    def ask_question(query):
        """
        This function takes a user's question, invokes the retrieval chain,
        and prints the answer along with the sources.
        """
        print("\n" + "="*50)
        print(f"Query: {query}")
        print("="*50)

        response = retrieval_chain.invoke({"input": query})

        print("\nAnswer:")
        print(response["answer"])

        # Print the sources used for the answer for transparency
        print("\n--- Sources ---")
        sources = set()
        for i, doc in enumerate(response["context"]):
            source_file = doc.metadata.get('source', 'Unknown')
            if source_file not in sources:
                 print(f"Source {len(sources) + 1}: {os.path.basename(source_file)}, Page: {doc.metadata.get('page', 'N/A')}")
                 sources.add(source_file)
            if len(sources) >= 3: # Limit to showing top 3 unique sources to avoid clutter
                 break
        print("="*50 + "\n")

    # Step 9: Ask Your Questions!
  
    # You can now ask any question related to the content of your textbooks.
    # The system will find the relevant parts and generate a specific answer.

    ask_question("According to Bentham “every law may be considered in eight different aspects”. Discuss")
    ask_question("Article 16 qualifies equality of opportunity in matters of public employment’. However there are certain exceptions to it. Discuss")
    ask_question("No one shall be vexed twice for the same cause. Explain the principle and the requirements for its application.")
