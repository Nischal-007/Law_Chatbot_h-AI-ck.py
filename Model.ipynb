!pip install llama-cpp-python langchain
!pip install langchain-community
!wget -O mistral.gguf https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf
from langchain.llms import LlamaCpp

llm = LlamaCpp(
    model_path="/content/mistral.gguf",
    n_ctx=2048,
    temperature=0.3,
    top_p=0.95,
    verbose=False,
    n_gpu_layers=20  # Set >0 if using Colab Pro GPU
)
